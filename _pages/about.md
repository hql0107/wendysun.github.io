---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
I'm a second-year graduate student at the [School of Automation](https://automation.seu.edu.cn/), [Southeast University](https://www.seu.edu.cn/). My research interests include computer vision, 3D vision, and embodied AI, with a current focus on 3D hand-object interaction and physics-based grasping.

I am very fortunate to be advised by [Prof. Yangang Wang](https://www.yangangwang.com/). Before that, I received my bachelor's degree from the [University of Electronic Science and Technology of China (UESTC)](https://www.uestc.edu.cn/).

[Email](mailto:wendysun0107@gmail.com) / [Github](https://github.com/hql0107)

Publications
======

<div style="display: flex; align-items: flex-start;">
  <div style="flex: 1;">
    <img src="images/GraspDiff.png" alt="Paper image" style="max-width: 100%; height: auto;">
  </div>
  <div style="flex: 3; padding-left: 15px;">
    <strong>GraspDiff: Grasping Generation for Hand-Object Interaction With Multimodal Guided Diffusion</strong><br>
    Binghui Zuo, Zimeng Zhao, <strong>Wenqian Sun</strong>, Xiaohan Yuan, Zhipeng Yu and Yangang Wang<br>
    <i>IEEE Transactions on Visualization and Computer Graphics (TVCG)</i><br>
<!--     2015-10-01<br><br> -->
    [<a href="https://ieeexplore.ieee.org/document/10689328">Paper</a>]
  </div>
</div>

---

<div style="display: flex; align-items: flex-start;">
  <div style="flex: 1;">
    <img src="images/pose.png" alt="Paper image" style="max-width: 100%; height: auto;">
  </div>
  <div style="flex: 3; padding-left: 15px;">
    <strong>Accurate and Real-time Variant Hand Pose Estimation Based on Gray Code Bounding Box Representation</strong><br>
    Yangang Wang, <strong>Wenqian Sun</strong>, and Ruting Rao<br>
    <i>IEEE Sensors Journal, 2024</i><br>
<!--     2010-10-01<br><br> -->
    [<a href="https://ieeexplore.ieee.org/document/10506333">Paper</a>]
  </div>
</div>

---

<div style="display: flex; align-items: flex-start;">
  <div style="flex: 1;">
    <img src="images/Interprior.png" alt="Paper image" style="max-width: 100%; height: auto;">
  </div>
  <div style="flex: 3; padding-left: 15px;">
    <strong>Reconstructing Interacting Hands with Interaction Prior from Monocular Images</strong><br>
    Binghui Zuo, Zimeng Zhao, <strong>Wenqian Sun</strong>, Wei Xie, Zhou Xue and Yangang Wang<br>
    <i>IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>), 2023</i><br>
<!--    The key idea is first to construct a two-hand interaction prior and recast the interaction reconstruction task as the conditional sampling from the prior.<br><br> -->
    [<a href="https://arxiv.org/abs/2308.14082">Paper</a>] [<a href="https://www.yangangwang.com/papers/iccv2023_interprior/BinghuiZuo-ICCV2023_InterPrior.html">Project Page</a>] [<a href="https://github.com/binghui-z/InterPrior_pytorch">Code [comming soon]</a>]
  </div>
</div>

---

Projects
======
<div class="container">
    <div class="image">
        <img src="path-to-your-project-image.png" alt="Project Image" style="max-width: 100%; height: auto;">
    </div>
    <div class="content" style="flex: 3; padding-left: 15px;">
        <strong>Motion Synthesis Framework for Hands-Object Interaction</strong><br>
        This project addresses <strong>hand-object interaction</strong> and proposes a two-stage motion synthesis framework for <strong>two hands and articulated objects</strong>. <br>
<!--           The first stage synthesizes two-hand motion based on a target state, while the second combines hand motion with the target to generate aligned object motion. We also introduce a module to enhance interaction patterns and use bounding boxes for object representation, making the framework versatile.<br> -->     
    </div>
</div>

<section>
    <h2>Project Demo Video</h2>
    <video width="720" height="480" controls>
        <source src="http://path-to-your-demo-video.mp4" type="video/mp4">
        Your browser does not support the video tag. Please <a href="http://path-to-your-demo-video.mp4">click here to watch the video</a>.
    </video>
</section>


